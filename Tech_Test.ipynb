{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech test\n",
    "\n",
    "The aim of this test is to evaluate some of the skills that you will use on your day-to-day activies at Sensyne Health.\n",
    "We collaborate as a team and the output of the Analytics side of the team has to be usable by others who might not necessarily be fluent in ML-ese.\n",
    "The aim of this task is to complete the assignment by focussing on key elements such as code reusability, clarity, conciseness, and use of best practices.\n",
    "\n",
    "In order to complete this assignment please consider the following classification problem given the dataset below (you are free to add and remove steps as you feel is required). \n",
    "\n",
    "Data contains information about mothers who may or may not develop diabetes (Outcome).\n",
    "\n",
    "1. Explore the data, identify and clarify any assumption you will make\n",
    "2. Consider any change/operation you will do based on your assumptions\n",
    "3. Your colleagues have used a Logistic regression classifier. Review the code and apply all the changes that you feel are required\n",
    "4. Compare this outcome with other two classifiers. Which one is the best out of the three?\n",
    "5. You are afraid of overfitting. How do you adjust your program to take care of that?\n",
    "6. Which classifier would you pick?\n",
    "\n",
    "At every step, git commit a different version of the Notebook to show the changes. Please do so on a local git repository. Don't worry about branches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "np.random.seed(int(rnd.random()*10000000))\n",
    "dataset = pd.read_csv(\"./dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Can you please explore the data and provide some valid assumptions on them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main assumptions I am making is that the samples are independent.  There really isn't a good way for me to test this based on the features provided.\n",
    "\n",
    "There are a lot of missing values for some of the features.  One assumption is that these data are missing at random.  A full discussion of this problem is outside the scope of this tech test, but we can look to see how these values are distributed with respect to the outcome variable (which itself is somewhat imbalanced):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupby('Outcome').agg(lambda x: len(x) - x.astype(bool).sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, these missing values (0s in this dataset) are distributed roughly in proportion to the outcome measure (about 2 to 1--i.e., there are about twice as many non-diabetes as diabetes outcomes).\n",
    "\n",
    "Looking at histograms of the individual features, we can see that a number of them are not normally distributed--though this is not a problem for logistic regression, it is for the regularization if used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dataset.columns:\n",
    "    fig,ax=plt.subplots()\n",
    "    dataset[col].plot.hist(ax=ax)\n",
    "    ax.set_xlabel(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another assumption of logistic regression is no multicollinearity.  We can some idea of this by looking at the heatmap of correlations between features.  We drop the outcome as we will look at that using point-biserial correlation.  Because of the large number of zeros, we really need to do some sort of imputation before we look at this, because we will be feeding our models imputed data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer()\n",
    "cols_with_missing = ['Glucose', 'BloodPressure', 'BMI', 'Insulin', 'SkinThickness']\n",
    "dataset.loc[:,cols_with_missing] = dataset.loc[:,cols_with_missing].replace({0:np.nan})\n",
    "dataset_arr = imp.fit_transform(dataset)\n",
    "dataset.loc[:,:] = dataset_arr\n",
    "corr_mat = dataset.drop(columns=['Outcome']).corr()\n",
    "sns.heatmap(corr_mat, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a high and somewhat understandable relationship between Age and number of pregnancies.  Otherwise, no serious problems.  We can test for multicollinearity by looking at the variance inflation factor (VIF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = dataset.drop(columns=['Outcome'])\n",
    "X.loc[:,'const'] = 1\n",
    "\n",
    "vif = pd.Series([variance_inflation_factor(X.values, i) \n",
    "               for i in range(X.shape[1])], \n",
    "              index=X.columns)\n",
    "\n",
    "vif.drop('const')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are all completely fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "for col in dataset.columns:\n",
    "    r,p = stats.pointbiserialr(dataset['Outcome'].values, dataset[col].values)\n",
    "    print('{}: r value is {}, p={}'.format(col,r,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all of the predictors have a significant relationship to the outcome variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "Anything that we need to do based on your assumptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply PowerTransform to Age, Pedigree, Insulin and Skin Thickness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class0 = dataset.Outcome==0\n",
    "class1 = dataset.Outcome==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_class0 = dataset[class0]\n",
    "data_class1 = dataset[class1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "# Remaining of the proportion get you (1 - 0.7) automatically\n",
    "\n",
    "train_split_0 = int(np.floor(0.7 * len(data_class0)))\n",
    "train_split_1 = int(np.floor(0.7 * len(data_class1)))\n",
    "\n",
    "train_data = pd.concat([ data_class0[ :train_split_0    ], data_class1[ :train_split_1    ] ])\n",
    "test_data  = pd.concat([ data_class0[  train_split_0+1: ], data_class1[  train_split_1+1: ] ])\n",
    "\n",
    "assert abs(0.7 - (len(train_data) / (len(train_data) + len(test_data)))) < 0.01, \"There must be a problem with the train/test split of data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "classifier = LogisticRegression().fit(train_data.loc[:, train_data.columns != 'Outcome'], np.ravel(train_data.loc[:, train_data.columns == 'Outcome']))\n",
    "\n",
    "prediction_test             = classifier.predict(test_data.loc[:, test_data.columns != 'Outcome'])\n",
    "prediction_probability_test = classifier.predict_proba(test_data.loc[:, test_data.columns != 'Outcome'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "metrics.plot_roc_curve(classifier, test_data.loc[:, test_data.columns != 'Outcome'], np.ravel(test_data.loc[:, test_data.columns == 'Outcome']), name= \"test data\", ax = ax)\n",
    "metrics.plot_roc_curve(classifier, train_data.loc[:, train_data.columns != 'Outcome'], np.ravel(train_data.loc[:, train_data.columns == 'Outcome']), name= \"train data\",ax = ax)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nteract": {
   "version": "0.21.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
